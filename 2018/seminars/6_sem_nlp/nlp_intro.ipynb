{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка естественной речи\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key words:** \n",
    "   nlp, spacy, word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Plan </h3>\n",
    "  * ** Напоминание возможных постановок задач nlp ** (10 minutes)\n",
    "  * ** Word2vec ** (30 minutes)\n",
    "  * ** Предобработка при помощи SpaCy ** (30 minutes)\n",
    "  * ** Примеры задач nlp *** (20 minutes)</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "По мотивам http://nlpx.net/archives/179 и https://github.com/danielfrg/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec — это инструмент (набор алгоритмов) для расчета векторных представлений слов, реализует две основные архитектуры — **Continuous Bag of Words (CBOW)** и **Skip-gram**. На вход подается корпус текста, а на выходе получается набор векторов слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более формально задача стоит так: **максимизация косинусной близости** между векторами слов (скалярное произведение векторов), которые появляются рядом друг с другом, и минимизация косинусной близости между векторами слов, которые не появляются друг рядом с другом. Рядом друг с другом в данном случае значит в близких контекстах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Напоминание:** косинусная близость\n",
    "    $$\\text{similarity} = \\cos(\\theta) = {A \\cdot B \\over \\|A\\| \\|B\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i \\times B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{(A_i)^2}} \\times \\sqrt{\\sum\\limits_{i=1}^{n}{(B_i)^2}} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec это НЕ глубокое обучение. Потому как там применяется обычная, а не «глубокая» нейросеть прямого распространения (Feed-forward Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще, CBOW и Skip-gram — это нейросетевые архитектуры, которые описывают, как именно нейросеть «учится» на данных и «запоминает» представления слов. Принципы у обоих архитектур разные. Принцип работы CBOW — предсказывание слова при данном контексте, а skip-gram наоборот — предсказывается контекст при данном слове.\n",
    "\n",
    "<img src=\"./word2vec.png\">\n",
    "\n",
    "- **Continuous Bag of Words (CBOW)** – обычная модель мешка слов с учётом четырёх ближайших соседей термина (два предыдущих и два последующих слова) без учёта порядка следования.\n",
    "\n",
    "- **k-skip-n-gram** — это последовательность длиной n, где элементы находятся на расстоянии не более, чем k друг от друга. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Негативное сэмплирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача построения модели word2vec: максимизация близости векторов слов (скалярное произведение векторов), которые появляются рядом друг с другом, и минимизация близости векторов слов, которые не появляются друг рядом с другом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим упрощенное уравнение этой идеи:\n",
    "    $$\n",
    "        \\frac{v_c\\times v_t}{\\sum v_{c1}\\times v_t}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В числителе мы имеем близость слов контекста ($v_{c}$) и целевого слова ($v_{t}$), в знаменателе — близость всех других контекстов ($v_{c1}$) и целевого слова ($v_{t}$). Проблема в том, что считать все это долго и сложно. Негативное сэмплирование: мы не считаем ВСЕ возможные контексты, а выбираем случайным образом НЕСКОЛЬКО контекстов $v_{c1}$. Такой подход значительно облегчает процесс тренировки word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немного практики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем данные: [http://mattmahoney.net/dc/text8.zip](http://mattmahoney.net/dc/text8.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file text8\n",
      "Words processed: 17000K     Vocab size: 4399K  rocessed: 2800K     Vocab size: 1104K  \n",
      "Vocab size (unigrams + bigrams): 2419827\n",
      "Words in train file: 17005206\n",
      "Words written: 17000K\r"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase('text8', 'text8-phrases', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем файл `text8-phrases`, чтобы подать его на вход  `word2vec`. Этот шаг можно пропустить и подать на вход сырые данные.\n",
    "\n",
    "Обучаем word2vec модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file text8-phrases\n",
      "Vocab size: 98331\n",
      "Words in train file: 15857306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.003435  Progress: 86.27%  Words/thread/sec: 264.82k  rds/thread/sec: 254.16k  read/sec: 254.57k  c: 256.06k  pha: 0.024463  Progress: 2.16%  Words/thread/sec: 256.68k  024408  Progress: 2.38%  Words/thread/sec: 258.36k   Progress: 2.60%  Words/thread/sec: 258.69k  ss: 2.82%  Words/thread/sec: 259.58k  3%  Words/thread/sec: 262.17k  ds/thread/sec: 260.05k  ad/sec: 260.89k   262.14k  k  ha: 0.023967  Progress: 4.15%  Words/thread/sec: 262.08k  23912  Progress: 4.36%  Words/thread/sec: 264.16k  : 4.80%  Words/thread/sec: 262.80k    Words/thread/sec: 263.01k  /thread/sec: 263.13k  /sec: 263.05k   0.023472  Progress: 6.12%  Words/thread/sec: 262.69k  18  Progress: 6.34%  Words/thread/sec: 263.40k  gress: 6.56%  Words/thread/sec: 263.27k  6.78%  Words/thread/sec: 262.98k  Words/thread/sec: 263.10k  c: 263.93k  9k  lpha: 0.023032  Progress: 7.89%  Words/thread/sec: 263.85k  .022977  Progress: 8.11%  Words/thread/sec: 262.57k    Progress: 8.32%  Words/thread/sec: 262.80k  ess: 8.54%  Words/thread/sec: 264.01k  76%  Words/thread/sec: 263.44k  ad/sec: 263.60k   263.26k  a: 0.022536  Progress: 9.87%  Words/thread/sec: 263.85k  : 0.022482  Progress: 10.09%  Words/thread/sec: 263.82k  k  c: 264.01k  thread/sec: 263.36k  %  Words/thread/sec: 263.51k  ss: 11.16%  Words/thread/sec: 263.77k   0.022104  Progress: 11.60%  Words/thread/sec: 263.44k   263.21k  read/sec: 263.86k   Words/thread/sec: 263.82k  : 12.67%  Words/thread/sec: 263.35k   Progress: 12.89%  Words/thread/sec: 263.97k  0.021726  Progress: 13.11%  Words/thread/sec: 264.03k   ead/sec: 264.11k  Words/thread/sec: 263.57k  rogress: 14.40%  Words/thread/sec: 263.68k  021348  Progress: 14.62%  Words/thread/sec: 263.68k  d/sec: 263.62k  rds/thread/sec: 263.95k  gress: 15.92%  Words/thread/sec: 263.59k  0970  Progress: 16.14%  Words/thread/sec: 263.95k  pha: 0.020915  Progress: 16.35%  Words/thread/sec: 264.27k  sec: 263.98k  s/thread/sec: 264.02k  21%  Words/thread/sec: 263.40k  ress: 17.43%  Words/thread/sec: 264.07k  590  Progress: 17.65%  Words/thread/sec: 263.73k  ha: 0.020536  Progress: 17.87%  Words/thread/sec: 263.71k  72k  sec: 264.27k  s/thread/sec: 263.44k  ess: 18.96%  Words/thread/sec: 264.22k  : 0.020157  Progress: 19.39%  Words/thread/sec: 263.95k  : 263.78k   Words/thread/sec: 264.06k  : 20.55%  Words/thread/sec: 264.28k   Progress: 20.78%  Words/thread/sec: 264.55k  0.019753  Progress: 21.00%  Words/thread/sec: 264.20k   ead/sec: 264.03k  22.08%  Words/thread/sec: 264.35k  19375  Progress: 22.51%  Words/thread/sec: 264.03k  .05k  s/thread/sec: 264.20k  ess: 23.81%  Words/thread/sec: 264.14k  : 0.018941  Progress: 24.25%  Words/thread/sec: 264.12k  : 264.03k   Words/thread/sec: 264.12k  : 25.32%  Words/thread/sec: 264.17k   Progress: 25.54%  Words/thread/sec: 264.06k  0.018563  Progress: 25.76%  Words/thread/sec: 264.22k    264.25k  read/sec: 264.27k   Words/thread/sec: 264.28k  Progress: 27.06%  Words/thread/sec: 264.13k  .018184  Progress: 27.28%  Words/thread/sec: 263.82k  264.06k  ead/sec: 264.13k  Words/thread/sec: 264.07k  rogress: 28.57%  Words/thread/sec: 264.08k  lpha: 0.017752  Progress: 29.01%  Words/thread/sec: 263.99k  /sec: 264.16k  87%  Words/thread/sec: 263.93k  ress: 30.09%  Words/thread/sec: 264.21k  a: 0.017372  Progress: 30.53%  Words/thread/sec: 264.26k  c: 264.33k  thread/sec: 263.99k  %  Words/thread/sec: 264.26k    Progress: 31.82%  Words/thread/sec: 264.13k   ead/sec: 264.25k  Words/thread/sec: 264.18k  rogress: 33.33%  Words/thread/sec: 264.28k  lpha: 0.016560  Progress: 33.77%  Words/thread/sec: 264.22k  4.25k  d/sec: 264.14k  rds/thread/sec: 264.18k  4.63%  Words/thread/sec: 264.18k  ogress: 34.85%  Words/thread/sec: 264.27k  16237  Progress: 35.07%  Words/thread/sec: 264.18k  lpha: 0.016183  Progress: 35.28%  Words/thread/sec: 264.16k  4.10k  d/sec: 264.38k  .14%  Words/thread/sec: 264.20k  gress: 36.36%  Words/thread/sec: 264.29k  5859  Progress: 36.58%  Words/thread/sec: 264.23k  pha: 0.015804  Progress: 36.80%  Words/thread/sec: 264.19k  .39k  /sec: 264.19k  ds/thread/sec: 264.24k  .66%  Words/thread/sec: 264.45k  gress: 37.88%  Words/thread/sec: 264.19k  5478  Progress: 38.10%  Words/thread/sec: 264.38k  25k  /thread/sec: 264.49k  ss: 39.41%  Words/thread/sec: 264.45k   0.015027  Progress: 39.92%  Words/thread/sec: 264.40k   264.54k  read/sec: 264.47k   41.02%  Words/thread/sec: 264.70k  014638  Progress: 41.46%  Words/thread/sec: 264.61k  4.46k  d/sec: 264.62k  .55%  Words/thread/sec: 264.51k  258  Progress: 42.98%  Words/thread/sec: 264.56k  2k  ec: 264.50k  %  Words/thread/sec: 264.66k  ss: 44.29%  Words/thread/sec: 264.48k   0.013823  Progress: 44.72%  Words/thread/sec: 264.64k   264.57k  read/sec: 264.46k   45.80%  Words/thread/sec: 264.51k  013444  Progress: 46.24%  Words/thread/sec: 264.46k  4.46k  ds/thread/sec: 264.44k  ress: 47.53%  Words/thread/sec: 264.47k  a: 0.013012  Progress: 47.97%  Words/thread/sec: 264.34k  c: 264.42k  thread/sec: 264.41k  s: 49.05%  Words/thread/sec: 264.41k  0.012632  Progress: 49.49%  Words/thread/sec: 264.52k  264.49k  ords/thread/sec: 264.34k  ogress: 50.79%  Words/thread/sec: 264.52k  pha: 0.012198  Progress: 51.22%  Words/thread/sec: 264.46k  sec: 264.43k  8%  Words/thread/sec: 264.52k  ess: 52.30%  Words/thread/sec: 264.39k  : 0.011819  Progress: 52.74%  Words/thread/sec: 264.60k  : 264.45k   Words/thread/sec: 264.36k  Progress: 54.03%  Words/thread/sec: 264.60k  .011440  Progress: 54.25%  Words/thread/sec: 264.43k  64.63k  ad/sec: 264.64k  5.33%  Words/thread/sec: 264.58k  ogress: 55.54%  Words/thread/sec: 264.66k  pha: 0.011009  Progress: 55.98%  Words/thread/sec: 264.64k  sec: 264.61k  4%  Words/thread/sec: 264.52k  5  Progress: 57.27%  Words/thread/sec: 264.65k    : 264.64k   Words/thread/sec: 264.58k  Progress: 58.80%  Words/thread/sec: 264.65k  d/sec: 264.61k  : 0.009796  Progress: 60.83%  Words/thread/sec: 264.78k  : 264.71k   Words/thread/sec: 264.89k  : 61.91%  Words/thread/sec: 264.66k  .009418  Progress: 62.34%  Words/thread/sec: 264.89k  64.86k  ad/sec: 264.87k  3.44%  Words/thread/sec: 264.89k  pha: 0.008942  Progress: 64.24%  Words/thread/sec: 264.91k  ec: 264.84k  /thread/sec: 264.91k  6%  Words/thread/sec: 264.96k  ess: 65.18%  Words/thread/sec: 264.91k  55  Progress: 65.40%  Words/thread/sec: 264.94k  a: 0.008600  Progress: 65.62%  Words/thread/sec: 265.01k  7k  ec: 264.89k  %  Words/thread/sec: 264.95k  ss: 66.70%  Words/thread/sec: 264.86k   ords/thread/sec: 264.88k  68.21%  Words/thread/sec: 264.93k  rogress: 68.43%  Words/thread/sec: 264.83k  007842  Progress: 68.65%  Words/thread/sec: 264.87k  d/sec: 264.89k  rds/thread/sec: 264.92k  9.72%  Words/thread/sec: 264.85k  ogress: 69.94%  Words/thread/sec: 264.88k  07464  Progress: 70.16%  Words/thread/sec: 264.91k  lpha: 0.007409  Progress: 70.38%  Words/thread/sec: 264.86k  4%  Words/thread/sec: 264.88k  ess: 71.46%  Words/thread/sec: 264.89k  85  Progress: 71.67%  Words/thread/sec: 265.05k  a: 0.007030  Progress: 71.89%  Words/thread/sec: 264.77k  6k  ec: 264.96k  /thread/sec: 264.87k  ss: 72.97%  Words/thread/sec: 264.88k  6  Progress: 73.19%  Words/thread/sec: 264.84k  : 0.006651  Progress: 73.41%  Words/thread/sec: 264.92k  k  rogress: 74.71%  Words/thread/sec: 264.92k  lpha: 0.006251  Progress: 75.14%  Words/thread/sec: 265.17k  4.99k  ds/thread/sec: 264.84k  .00%  Words/thread/sec: 264.99k  892  Progress: 76.44%  Words/thread/sec: 264.91k  7k  ec: 264.87k  %  Words/thread/sec: 265.09k  98k  264.96k  ords/thread/sec: 264.93k  ogress: 79.49%  Words/thread/sec: 265.03k  05071  Progress: 79.73%  Words/thread/sec: 264.98k  lpha: 0.005013  Progress: 79.97%  Words/thread/sec: 265.00k  4.95k  d/sec: 265.06k  rds/thread/sec: 264.98k  0.85%  Words/thread/sec: 264.93k  ogress: 81.07%  Words/thread/sec: 264.86k  ec: 264.84k  %  Words/thread/sec: 265.01k    Progress: 82.80%  Words/thread/sec: 264.90k   0.004248  Progress: 83.02%  Words/thread/sec: 264.92k    : 264.87k  hread/sec: 264.93k    Words/thread/sec: 264.85k  s: 84.11%  Words/thread/sec: 264.85k  0.003865  Progress: 84.55%  Words/thread/sec: 264.80k  264.94k  2%  Words/thread/sec: 264.88k  ha: 0.003431  Progress: 86.29%  Words/thread/sec: 264.86k  \r",
      "Alpha: 0.003428  Progress: 86.30%  Words/thread/sec: 264.88k  \r",
      "Alpha: 0.003424  Progress: 86.32%  Words/thread/sec: 264.90k  \r",
      "Alpha: 0.003421  Progress: 86.33%  Words/thread/sec: 264.91k  \r",
      "Alpha: 0.003417  Progress: 86.34%  Words/thread/sec: 264.95k  \r",
      "Alpha: 0.003414  Progress: 86.36%  Words/thread/sec: 264.81k  \r",
      "Alpha: 0.003411  Progress: 86.37%  Words/thread/sec: 264.77k  \r",
      "Alpha: 0.003407  Progress: 86.38%  Words/thread/sec: 264.74k  \r",
      "Alpha: 0.003404  Progress: 86.40%  Words/thread/sec: 264.76k  \r",
      "Alpha: 0.003400  Progress: 86.41%  Words/thread/sec: 264.79k  \r",
      "Alpha: 0.003397  Progress: 86.43%  Words/thread/sec: 264.80k  \r",
      "Alpha: 0.003393  Progress: 86.44%  Words/thread/sec: 264.84k  \r",
      "Alpha: 0.003390  Progress: 86.45%  Words/thread/sec: 264.87k  \r",
      "Alpha: 0.003386  Progress: 86.47%  Words/thread/sec: 264.89k  \r",
      "Alpha: 0.003383  Progress: 86.48%  Words/thread/sec: 264.92k  \r",
      "Alpha: 0.003380  Progress: 86.49%  Words/thread/sec: 264.92k  \r",
      "Alpha: 0.003376  Progress: 86.51%  Words/thread/sec: 264.96k  \r",
      "Alpha: 0.003373  Progress: 86.52%  Words/thread/sec: 264.81k  \r",
      "Alpha: 0.003370  Progress: 86.54%  Words/thread/sec: 264.78k  \r",
      "Alpha: 0.003366  Progress: 86.55%  Words/thread/sec: 264.76k  \r",
      "Alpha: 0.003362  Progress: 86.56%  Words/thread/sec: 264.77k  \r",
      "Alpha: 0.003359  Progress: 86.58%  Words/thread/sec: 264.80k  \r",
      "Alpha: 0.003356  Progress: 86.59%  Words/thread/sec: 264.82k  \r",
      "Alpha: 0.003352  Progress: 86.61%  Words/thread/sec: 264.85k  \r",
      "Alpha: 0.003349  Progress: 86.62%  Words/thread/sec: 264.89k  \r",
      "Alpha: 0.003345  Progress: 86.63%  Words/thread/sec: 264.92k  \r",
      "Alpha: 0.003342  Progress: 86.65%  Words/thread/sec: 264.94k  \r",
      "Alpha: 0.003338  Progress: 86.66%  Words/thread/sec: 264.94k  \r",
      "Alpha: 0.003335  Progress: 86.67%  Words/thread/sec: 264.95k  \r",
      "Alpha: 0.003331  Progress: 86.69%  Words/thread/sec: 264.80k  \r",
      "Alpha: 0.003328  Progress: 86.70%  Words/thread/sec: 264.76k  \r",
      "Alpha: 0.003325  Progress: 86.71%  Words/thread/sec: 264.76k  \r",
      "Alpha: 0.003321  Progress: 86.73%  Words/thread/sec: 264.77k  \r",
      "Alpha: 0.003318  Progress: 86.74%  Words/thread/sec: 264.78k  \r",
      "Alpha: 0.003318  Progress: 86.76%  Words/thread/sec: 264.82k  \r",
      "Alpha: 0.003318  Progress: 86.77%  Words/thread/sec: 264.85k  \r",
      "Alpha: 0.003318  Progress: 86.78%  Words/thread/sec: 264.88k  \r",
      "Alpha: 0.003318  Progress: 86.80%  Words/thread/sec: 264.91k  \r",
      "Alpha: 0.003318  Progress: 86.81%  Words/thread/sec: 264.95k  \r",
      "Alpha: 0.003318  Progress: 86.82%  Words/thread/sec: 264.99k  \r",
      "Alpha: 0.003294  Progress: 86.84%  Words/thread/sec: 264.91k  \r",
      "Alpha: 0.003290  Progress: 86.85%  Words/thread/sec: 264.78k  \r",
      "Alpha: 0.003287  Progress: 86.87%  Words/thread/sec: 264.77k  \r",
      "Alpha: 0.003284  Progress: 86.88%  Words/thread/sec: 264.78k  \r",
      "Alpha: 0.003280  Progress: 86.89%  Words/thread/sec: 264.80k  \r",
      "Alpha: 0.003277  Progress: 86.91%  Words/thread/sec: 264.82k  \r",
      "Alpha: 0.003274  Progress: 86.92%  Words/thread/sec: 264.82k  \r",
      "Alpha: 0.003270  Progress: 86.93%  Words/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.000002  Progress: 100.03%  Words/thread/sec: 265.00k  s/thread/sec: 264.78k  : 0.002620  Progress: 89.53%  Words/thread/sec: 264.81k  Words/thread/sec: 264.74k  pha: 0.002187  Progress: 91.27%  Words/thread/sec: 264.79k  %  Words/thread/sec: 264.75k  .86%  Words/thread/sec: 264.80k  k   95.58%  Words/thread/sec: 264.78k  .79k  ss: 97.31%  Words/thread/sec: 264.67k  264.82k  gress: 99.08%  Words/thread/sec: 264.79k  c: 264.98k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec('text8-phrases', 'text8.bin', size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл `text8.bin` содержит вектора слов в бинарном виде."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем кластеры на основе обученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file text8\n",
      "Vocab size: 71291\n",
      "Words in train file: 16718843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.003234  Progress: 87.10%  Words/thread/sec: 262.55k  259.09k  a: 0.024515  Progress: 1.95%  Words/thread/sec: 260.12k  4464  Progress: 2.16%  Words/thread/sec: 263.30k  rogress: 2.36%  Words/thread/sec: 261.98k  : 2.57%  Words/thread/sec: 262.17k  thread/sec: 263.76k  sec: 264.52k  4.60k  0  Progress: 4.01%  Words/thread/sec: 265.14k  42%  Words/thread/sec: 265.34k  ad/sec: 265.98k    587  Progress: 5.67%  Words/thread/sec: 266.71k  ogress: 5.87%  Words/thread/sec: 266.51k   6.07%  Words/thread/sec: 265.51k   Words/thread/sec: 266.58k  thread/sec: 266.78k  .40k  .023176  Progress: 7.31%  Words/thread/sec: 267.58k  ss: 7.72%  Words/thread/sec: 266.96k  s/thread/sec: 266.15k  d/sec: 267.50k   : 0.022764  Progress: 8.96%  Words/thread/sec: 268.00k  713  Progress: 9.16%  Words/thread/sec: 266.83k  ogress: 9.36%  Words/thread/sec: 266.85k   9.57%  Words/thread/sec: 266.97k   Words/thread/sec: 266.86k   Words/thread/sec: 267.24k  : 10.38%  Words/thread/sec: 267.46k   Progress: 10.59%  Words/thread/sec: 267.67k  267.51k  ords/thread/sec: 266.82k  ogress: 12.00%  Words/thread/sec: 266.71k  21953  Progress: 12.20%  Words/thread/sec: 267.37k  lpha: 0.021902  Progress: 12.41%  Words/thread/sec: 267.68k  /sec: 267.37k  ds/thread/sec: 267.66k  .21%  Words/thread/sec: 267.06k  gress: 13.41%  Words/thread/sec: 267.49k  1599  Progress: 13.62%  Words/thread/sec: 267.74k  pha: 0.021548  Progress: 13.82%  Words/thread/sec: 267.65k  .19k  /sec: 267.44k  ds/thread/sec: 267.21k  .63%  Words/thread/sec: 267.20k  gress: 14.83%  Words/thread/sec: 267.87k  1244  Progress: 15.04%  Words/thread/sec: 267.31k  pha: 0.021193  Progress: 15.24%  Words/thread/sec: 267.51k  sec: 267.37k  4%  Words/thread/sec: 267.84k  ess: 16.25%  Words/thread/sec: 267.23k  90  Progress: 16.45%  Words/thread/sec: 267.52k  a: 0.020838  Progress: 16.66%  Words/thread/sec: 267.77k  0k  ec: 267.37k  /thread/sec: 267.97k  7%  Words/thread/sec: 267.38k  ess: 17.67%  Words/thread/sec: 267.43k  34  Progress: 17.88%  Words/thread/sec: 267.50k  k  hread/sec: 267.54k  : 19.09%  Words/thread/sec: 267.56k  .020125  Progress: 19.51%  Words/thread/sec: 267.66k  ad/sec: 267.30k  ords/thread/sec: 267.41k  20.60%  Words/thread/sec: 267.60k  rogress: 20.81%  Words/thread/sec: 267.52k  019751  Progress: 21.01%  Words/thread/sec: 267.48k  d/sec: 267.25k  rds/thread/sec: 267.26k  gress: 22.22%  Words/thread/sec: 267.53k  9398  Progress: 22.42%  Words/thread/sec: 267.29k  pha: 0.019347  Progress: 22.62%  Words/thread/sec: 267.18k  sec: 267.25k  s/thread/sec: 267.02k  43%  Words/thread/sec: 267.48k  44  Progress: 23.84%  Words/thread/sec: 267.28k  a: 0.018992  Progress: 24.04%  Words/thread/sec: 267.47k  c: 267.46k    Words/thread/sec: 267.39k   Progress: 25.26%  Words/thread/sec: 267.55k  0.018637  Progress: 25.47%  Words/thread/sec: 267.66k  267.57k  ead/sec: 267.48k  Words/thread/sec: 267.73k   26.47%  Words/thread/sec: 267.61k  Progress: 26.68%  Words/thread/sec: 267.50k  .018283  Progress: 26.88%  Words/thread/sec: 267.33k  67.33k  rds/thread/sec: 267.56k  ec: 267.41k  ogress: 28.09%  Words/thread/sec: 267.49k  17929  Progress: 28.30%  Words/thread/sec: 267.60k  .36k  /sec: 267.48k  ds/thread/sec: 267.35k  .30%  Words/thread/sec: 267.37k  gress: 29.51%  Words/thread/sec: 267.41k  7575  Progress: 29.71%  Words/thread/sec: 267.19k  pha: 0.017524  Progress: 29.92%  Words/thread/sec: 267.48k  .48k  /sec: 267.22k  72%  Words/thread/sec: 267.49k  ress: 30.92%  Words/thread/sec: 267.23k  222  Progress: 31.13%  Words/thread/sec: 267.43k  ha: 0.017171  Progress: 31.33%  Words/thread/sec: 267.53k  27k  sec: 267.43k  s/thread/sec: 267.67k  13%  Words/thread/sec: 267.16k  ress: 32.33%  Words/thread/sec: 267.40k  869  Progress: 32.54%  Words/thread/sec: 267.31k  ha: 0.016817  Progress: 32.74%  Words/thread/sec: 267.30k  45k  sec: 267.59k  s/thread/sec: 267.24k  55%  Words/thread/sec: 267.46k  14  Progress: 33.96%  Words/thread/sec: 267.27k  k  hread/sec: 267.42k    Words/thread/sec: 267.71k  s: 35.17%  Words/thread/sec: 267.16k    Progress: 35.37%  Words/thread/sec: 267.51k   0.016108  Progress: 35.58%  Words/thread/sec: 267.52k    read/sec: 267.38k   Words/thread/sec: 267.03k  Progress: 36.79%  Words/thread/sec: 267.01k  d/sec: 266.31k  rds/thread/sec: 266.57k  8.01%  Words/thread/sec: 266.50k  5397  Progress: 38.42%  Words/thread/sec: 266.05k  pha: 0.015346  Progress: 38.63%  Words/thread/sec: 266.04k  .84k  s/thread/sec: 265.81k  47%  Words/thread/sec: 265.70k  ress: 39.69%  Words/thread/sec: 265.57k  a: 0.014975  Progress: 40.11%  Words/thread/sec: 265.30k  c: 265.23k    Words/thread/sec: 265.16k  s: 41.14%  Words/thread/sec: 265.02k  0.014615  Progress: 41.55%  Words/thread/sec: 264.88k   ead/sec: 264.71k  42.56%  Words/thread/sec: 264.38k  14261  Progress: 42.97%  Words/thread/sec: 264.18k  .22k  /sec: 264.13k  ds/thread/sec: 264.20k  .98%  Words/thread/sec: 264.03k  gress: 44.19%  Words/thread/sec: 263.98k  ha: 0.013855  Progress: 44.59%  Words/thread/sec: 263.92k  08k  sec: 263.99k  0%  Words/thread/sec: 263.92k  1  Progress: 45.81%  Words/thread/sec: 263.63k  : 0.013500  Progress: 46.01%  Words/thread/sec: 263.70k  hread/sec: 263.60k  : 47.02%  Words/thread/sec: 263.61k   Progress: 47.23%  Words/thread/sec: 263.71k  263.62k  ead/sec: 263.68k  Words/thread/sec: 263.60k   48.44%  Words/thread/sec: 263.80k  Progress: 48.64%  Words/thread/sec: 263.72k  .012793  Progress: 48.84%  Words/thread/sec: 263.61k  263.71k  ead/sec: 263.63k  2438  Progress: 50.26%  Words/thread/sec: 263.73k  pha: 0.012387  Progress: 50.46%  Words/thread/sec: 263.72k  sec: 263.73k  s/thread/sec: 263.78k  27%  Words/thread/sec: 263.65k  85  Progress: 51.67%  Words/thread/sec: 263.83k  k  c: 263.67k  thread/sec: 263.77k  .011680  Progress: 53.29%  Words/thread/sec: 263.83k  63.87k  rds/thread/sec: 263.81k  4.30%  Words/thread/sec: 263.93k  ogress: 54.50%  Words/thread/sec: 263.94k  pha: 0.011275  Progress: 54.91%  Words/thread/sec: 263.93k  .00k  s/thread/sec: 263.93k  72%  Words/thread/sec: 264.02k  71  Progress: 56.13%  Words/thread/sec: 263.91k  read/sec: 263.93k   57.34%  Words/thread/sec: 263.92k  Progress: 57.55%  Words/thread/sec: 263.96k  d/sec: 264.03k  .77%  Words/thread/sec: 264.13k  206  Progress: 59.20%  Words/thread/sec: 264.12k  hread/sec: 263.97k    Words/thread/sec: 264.05k  s: 60.45%  Words/thread/sec: 264.15k  0.009789  Progress: 60.86%  Words/thread/sec: 264.06k    264.09k  read/sec: 263.99k   61.87%  Words/thread/sec: 264.15k  Progress: 62.08%  Words/thread/sec: 264.07k  ds/thread/sec: 264.02k  11  Progress: 63.71%  Words/thread/sec: 264.34k  k  c: 264.04k  thread/sec: 263.96k  %  Words/thread/sec: 264.09k    Progress: 65.13%  Words/thread/sec: 263.91k   ead/sec: 263.91k  8316  Progress: 66.75%  Words/thread/sec: 263.94k  pha: 0.008277  Progress: 66.95%  Words/thread/sec: 263.93k  .87k  s/thread/sec: 263.84k  ess: 67.96%  Words/thread/sec: 263.83k  61  Progress: 68.17%  Words/thread/sec: 263.59k  k  c: 263.75k    Words/thread/sec: 263.63k  d/sec: 263.64k  .79%  Words/thread/sec: 263.83k  204  Progress: 71.20%  Words/thread/sec: 263.74k  ha: 0.007153  Progress: 71.40%  Words/thread/sec: 263.77k  ec: 263.76k  %  Words/thread/sec: 263.81k    Progress: 72.61%  Words/thread/sec: 263.84k  rds/thread/sec: 264.06k  3.82%  Words/thread/sec: 263.84k  6445  Progress: 74.23%  Words/thread/sec: 263.93k  72k  /thread/sec: 263.60k  ss: 75.45%  Words/thread/sec: 263.60k   0.006039  Progress: 75.86%  Words/thread/sec: 263.50k  ords/thread/sec: 263.28k  76.87%  Words/thread/sec: 263.25k  05684  Progress: 77.28%  Words/thread/sec: 263.20k  .05k  s/thread/sec: 262.94k  ess: 78.50%  Words/thread/sec: 262.85k  : 0.005274  Progress: 78.92%  Words/thread/sec: 262.81k  Words/thread/sec: 262.66k  04938  Progress: 80.40%  Words/thread/sec: 262.78k  .47k  s/thread/sec: 262.37k  ess: 81.62%  Words/thread/sec: 262.44k  : 0.004497  Progress: 82.04%  Words/thread/sec: 262.36k  k  c: 262.29k  : 83.05%  Words/thread/sec: 262.20k  d/sec: 262.26k  .47%  Words/thread/sec: 262.26k  783  Progress: 84.88%  Words/thread/sec: 262.37k  5k    Words/thread/sec: 262.30k   Progress: 86.30%  Words/thread/sec: 262.39k  d/sec: 262.57k  \r",
      "Alpha: 0.003224  Progress: 87.11%  Words/thread/sec: 262.55k  \r",
      "Alpha: 0.003224  Progress: 87.13%  Words/thread/sec: 262.58k  \r",
      "Alpha: 0.003218  Progress: 87.14%  Words/thread/sec: 262.57k  \r",
      "Alpha: 0.003215  Progress: 87.15%  Words/thread/sec: 262.46k  \r",
      "Alpha: 0.003212  Progress: 87.16%  Words/thread/sec: 262.43k  \r",
      "Alpha: 0.003209  Progress: 87.18%  Words/thread/sec: 262.34k  \r",
      "Alpha: 0.003206  Progress: 87.19%  Words/thread/sec: 262.34k  \r",
      "Alpha: 0.003202  Progress: 87.20%  Words/thread/sec: 262.37k  \r",
      "Alpha: 0.003199  Progress: 87.22%  Words/thread/sec: 262.39k  \r",
      "Alpha: 0.003196  Progress: 87.23%  Words/thread/sec: 262.40k  \r",
      "Alpha: 0.003193  Progress: 87.24%  Words/thread/sec: 262.42k  \r",
      "Alpha: 0.003190  Progress: 87.25%  Words/thread/sec: 262.45k  \r",
      "Alpha: 0.003187  Progress: 87.27%  Words/thread/sec: 262.47k  \r",
      "Alpha: 0.003184  Progress: 87.28%  Words/thread/sec: 262.51k  \r",
      "Alpha: 0.003180  Progress: 87.29%  Words/thread/sec: 262.53k  \r",
      "Alpha: 0.003177  Progress: 87.30%  Words/thread/sec: 262.44k  \r",
      "Alpha: 0.003174  Progress: 87.32%  Words/thread/sec: 262.45k  \r",
      "Alpha: 0.003171  Progress: 87.33%  Words/thread/sec: 262.35k  \r",
      "Alpha: 0.003168  Progress: 87.34%  Words/thread/sec: 262.38k  \r",
      "Alpha: 0.003165  Progress: 87.35%  Words/thread/sec: 262.40k  \r",
      "Alpha: 0.003161  Progress: 87.37%  Words/thread/sec: 262.38k  \r",
      "Alpha: 0.003158  Progress: 87.38%  Words/thread/sec: 262.42k  \r",
      "Alpha: 0.003155  Progress: 87.39%  Words/thread/sec: 262.44k  \r",
      "Alpha: 0.003152  Progress: 87.41%  Words/thread/sec: 262.46k  \r",
      "Alpha: 0.003149  Progress: 87.42%  Words/thread/sec: 262.48k  \r",
      "Alpha: 0.003145  Progress: 87.43%  Words/thread/sec: 262.51k  \r",
      "Alpha: 0.003142  Progress: 87.44%  Words/thread/sec: 262.53k  \r",
      "Alpha: 0.003139  Progress: 87.46%  Words/thread/sec: 262.45k  \r",
      "Alpha: 0.003136  Progress: 87.47%  Words/thread/sec: 262.45k  \r",
      "Alpha: 0.003133  Progress: 87.48%  Words/thread/sec: 262.37k  \r",
      "Alpha: 0.003130  Progress: 87.49%  Words/thread/sec: 262.39k  \r",
      "Alpha: 0.003126  Progress: 87.51%  Words/thread/sec: 262.40k  \r",
      "Alpha: 0.003123  Progress: 87.52%  Words/thread/sec: 262.43k  \r",
      "Alpha: 0.003120  Progress: 87.53%  Words/thread/sec: 262.44k  \r",
      "Alpha: 0.003117  Progress: 87.55%  Words/thread/sec: 262.46k  \r",
      "Alpha: 0.003114  Progress: 87.56%  Words/thread/sec: 262.48k  \r",
      "Alpha: 0.003110  Progress: 87.57%  Words/thread/sec: 262.52k  \r",
      "Alpha: 0.003107  Progress: 87.58%  Words/thread/sec: 262.53k  \r",
      "Alpha: 0.003104  Progress: 87.60%  Words/thread/sec: 262.55k  \r",
      "Alpha: 0.003101  Progress: 87.61%  Words/thread/sec: 262.44k  \r",
      "Alpha: 0.003098  Progress: 87.62%  Words/thread/sec: 262.46k  \r",
      "Alpha: 0.003095  Progress: 87.63%  Words/thread/sec: 262.37k  \r",
      "Alpha: 0.003092  Progress: 87.65%  Words/thread/sec: 262.39k  \r",
      "Alpha: 0.003088  Progress: 87.66%  Words/thread/sec: 262.43k  \r",
      "Alpha: 0.003085  Progress: 87.67%  Words/thread/sec: 262.44k  \r",
      "Alpha: 0.003082  Progress: 87.68%  Words/thread/sec: 262.46k  \r",
      "Alpha: 0.003079  Progress: 87.70%  Words/thread/sec: 262.46k  \r",
      "Alpha: 0.003076  Progress: 87.71%  Words/thread/sec: 262.48k  \r",
      "Alpha: 0.003073  Progress: 87.72%  Words/thread/sec: 262.50k  \r",
      "Alpha: 0.003069  Progress: 87.74%  Words/thread/sec: 262.53k  \r",
      "Alpha: 0.003066  Progress: 87.75%  Words/thread/sec: 262.52k  \r",
      "Alpha: 0.003063  Progress: 87.76%  Words/thread/sec: 262.49k  \r",
      "Alpha: 0.003060  Progress: 87.77%  Words/thread/sec: 262.48k  \r",
      "Alpha: 0.003057  Progress: 87.79%  Words/thread/sec: 262.37k  \r",
      "Alpha: 0.003053  Progress: 87.80%  Words/thread/sec: 262.40k  \r",
      "Alpha: 0.003050  Progress: 87.81%  Words/thread/sec: 262.43k  \r",
      "Alpha: 0.003047  Progress: 87.82%  Words/thread/sec: 262.45k  \r",
      "Alpha: 0.003044  Progress: 87.84%  Words/thread/sec: 262.47k  \r",
      "Alpha: 0.003041  Progress: 87.85%  Words/thread/sec: 262.47k  \r",
      "Alpha: 0.003038  Progress: 87.86%  Words/thread/sec: 262.49k  \r",
      "Alpha: 0.003034  Progress: 87.88%  Words/thread/sec: 262.49k  \r",
      "Alpha: 0.003031  Progress: 87.89%  Words/thread/sec: 262.50k  \r",
      "Alpha: 0.003028  Progress: 87.90%  Words/thread/sec: 262.53k  \r",
      "Alpha: 0.0030"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.000002  Progress: 100.03%  Words/thread/sec: 263.34k  2619  Progress: 89.53%  Words/thread/sec: 262.65k  /thread/sec: 262.73k  0.002215  Progress: 91.15%  Words/thread/sec: 262.56k  rds/thread/sec: 262.68k  a: 0.001811  Progress: 92.77%  Words/thread/sec: 262.80k   Words/thread/sec: 262.89k  lpha: 0.001405  Progress: 94.39%  Words/thread/sec: 262.92k  0%  Words/thread/sec: 262.89k   6.82%  Words/thread/sec: 263.05k  7k  : 98.46%  Words/thread/sec: 263.15k  3.31k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2clusters('text8', 'text8-clusters.txt', 100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл `text8-clusters.txt` содержит кластеры для каждого слова словаря."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.load('text8.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', 'the', 'of', ..., 'dakotas', 'nias', 'burlesques'],\n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся матрица:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98331, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14333282,  0.15825513, -0.13715845, ...,  0.05456942,\n",
       "         0.10955409,  0.00693387],\n",
       "       [ 0.10136448, -0.01455281,  0.10195114, ..., -0.10148882,\n",
       "         0.12464498, -0.02213244],\n",
       "       [ 0.18440248,  0.03617   ,  0.22163972, ...,  0.0482553 ,\n",
       "         0.09393772,  0.08885094],\n",
       "       ...,\n",
       "       [-0.10395487,  0.04674925,  0.12876071, ...,  0.12327843,\n",
       "        -0.09347397, -0.05557026],\n",
       "       [ 0.02035719,  0.03220995,  0.09305181, ...,  0.03481731,\n",
       "        -0.19664939, -0.12811995],\n",
       "       [-0.00415086, -0.04732238,  0.04623443, ...,  0.04687344,\n",
       "        -0.11005756, -0.07075066]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем узнать вектор любого слова из словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['dog'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11632952,  0.01694006,  0.08824241, -0.0306368 ,  0.15153386,\n",
       "       -0.05756098,  0.00574881,  0.09358197,  0.10525652, -0.07909904])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['dog'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посчитать попарные расстояния между двумя или несколькими словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 'cat', 0.8793564673119473),\n",
       " ('dog', 'fish', 0.5833656659468412),\n",
       " ('cat', 'fish', 0.6177639537413828)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distance(\"dog\", \"cat\", \"fish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сходство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2437,  5478,  7593, 10309,  2428,  2670,  2391,  3964,  4812,\n",
       "        10230]),\n",
       " array([0.87935647, 0.84306434, 0.7819305 , 0.77183196, 0.76222532,\n",
       "        0.75403152, 0.75380452, 0.75322427, 0.75230246, 0.7505155 ]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.similar(\"dog\")\n",
    "indexes, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе получаем два списка:\n",
    "1. список индексов слов, схожих с данным\n",
    "2. список значений сходства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat', 'cow', 'goat', 'rat', 'bear', 'bird', 'girl', 'dogs',\n",
       "       'wolf', 'pig'], dtype='<U78')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([('cat', 0.87935647), ('cow', 0.84306434), ('goat', 0.7819305 ),\n",
       "           ('rat', 0.77183196), ('bear', 0.76222532),\n",
       "           ('bird', 0.75403152), ('girl', 0.75380452),\n",
       "           ('dogs', 0.75322427), ('wolf', 0.75230246),\n",
       "           ('pig', 0.7505155 )],\n",
       "          dtype=[('word', '<U78'), ('metric', '<f8')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.8793564673119473),\n",
       " ('cow', 0.8430643421426257),\n",
       " ('goat', 0.7819305043099686),\n",
       " ('rat', 0.7718319569153317),\n",
       " ('bear', 0.7622253242697923),\n",
       " ('bird', 0.7540315175337027),\n",
       " ('girl', 0.7538045170919909),\n",
       " ('dogs', 0.75322426644677),\n",
       " ('wolf', 0.7523024639778775),\n",
       " ('pig', 0.7505155012900172)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фразы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку мы обучили алгоритм на `word2phrase` мы можем отобразить схотство исходя из \"фраз\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('san_francisco', 0.893406648007967),\n",
       " ('san_diego', 0.8728984083777882),\n",
       " ('las_vegas', 0.8418905944462175),\n",
       " ('miami', 0.8313320556508131),\n",
       " ('seattle', 0.8303476300600032),\n",
       " ('california', 0.8274332044660233),\n",
       " ('cleveland', 0.8266549604180308),\n",
       " ('detroit', 0.8210469809650927),\n",
       " ('chicago', 0.8184177165262507),\n",
       " ('cincinnati', 0.8164385911935804)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.similar('los_angeles')\n",
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналогии и линейность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./pict.jpg\">\n",
    " \n",
    " `king - man + woman = queen` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1087, 7523, 1145, 1335, 1827, 3141, 6768,  648, 8419, 4980]),\n",
       " array([0.29351067, 0.27462383, 0.27178875, 0.26943102, 0.2651588 ,\n",
       "        0.264542  , 0.2640163 , 0.26312415, 0.26281856, 0.25769584]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = model.analogy(pos=['king', 'woman'], neg=['man'])\n",
    "indexes, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.293510674759427),\n",
       " ('empress', 0.2746238293668638),\n",
       " ('prince', 0.2717887471461545),\n",
       " ('wife', 0.26943102115434225),\n",
       " ('throne', 0.2651588009936282),\n",
       " ('monarch', 0.2645420049766334),\n",
       " ('regent', 0.26401629581670216),\n",
       " ('emperor', 0.2631241459676974),\n",
       " ('aragon', 0.262818561874043),\n",
       " ('heir', 0.25769583841034893)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = word2vec.load_clusters('text8-clusters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['</s>', 'the', 'of', ..., 'bredon', 'skirting', 'santamaria'],\n",
       "      dtype='<U29')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посмотреть все слова, находящиеся в определенном кластере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.get_words_on_cluster(90).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['along', 'associated', 'working', 'relations', 'relationship',\n",
       "       'deal', 'compared', 'combined', 'contrast', 'contact'],\n",
       "      dtype='<U29')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.get_words_on_cluster(90)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем добавить кластеры в модель word2vec и генерироваь ответы, включающие кластеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clusters = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes, metrics = model.analogy(pos=[\"paris\", \"germany\"], neg=[\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('berlin', 0.32275486206528214, 15),\n",
       " ('munich', 0.2862855188930788, 21),\n",
       " ('vienna', 0.2760421477296262, 12),\n",
       " ('st_petersburg', 0.2712241488630337, 61),\n",
       " ('leipzig', 0.2692546127699573, 8),\n",
       " ('moscow', 0.26664249371504267, 74),\n",
       " ('dresden', 0.2546390773975281, 71),\n",
       " ('prague', 0.24980033046195801, 72),\n",
       " ('z_rich', 0.24818770928729006, 80),\n",
       " ('bonn', 0.24179059607436923, 77)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Еще несколько комментариев:**\n",
    "    \n",
    "- CBOW работает быстрее, зато Skip-gram работает лучше, особенно для относительно редких слов\n",
    "- Иерархический софтмакс хорошо подходит для создания лучшей модели относительно редких слов, негативное сэмплирование же лучше моделирует более частотные слова.\n",
    "- Применение суб-сэмплирования улучшает производительность. Рекомендуемый параметр субсэмплирования от 1e-3 до 1е-5\n",
    "- Размер вектора чем больше, тем лучше (впрочем, не всегда, это от корпуса зависит)\n",
    "- Размер окна — для Skip-gram оптимальный размер около 10, для CBOW — в районе 5\n",
    "\n",
    "**Одна из работающих схем:**\n",
    "\n",
    "skip-gram + negative sampling + окно 10 слов + субсэмплирование 1е-5 + размер вектора 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка при помощи spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy это Cython/Python библиотека для обработки текстов. Работает быстрее чем NLTK, имеет довольно широкие возможности. https://spacy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n",
      "be\n",
      "look\n",
      "at\n",
      "buy\n",
      "u.k.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение стоп-слов, парсинг зависимостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text: The original word text.\n",
    "- Lemma: The base form of the word.\n",
    "- POS: The simple part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag.\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "- Shape: The word shape – capitalisation, punctuation, digits.\n",
    "- is alpha: Is the token an alpha character?\n",
    "- is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1975\" height=\"399.5\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1800.0,2.0 1800.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1800.0,266.5 L1808.0,254.5 1792.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1700\" height=\"362.0\" style=\"max-width: none; height: 362.0px; color: white; background: #09a3d5; font-family: Source Sans Pro\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"800\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"800\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1550\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1550\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M62,227.0 62,177.0 347.0,177.0 347.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,229.0 L58,221.0 66,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M212,227.0 212,202.0 344.0,202.0 344.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M212,229.0 L208,221.0 216,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M362,227.0 362,202.0 494.0,202.0 494.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M494.0,229.0 L498.0,221.0 490.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M512,227.0 512,202.0 644.0,202.0 644.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M644.0,229.0 L648.0,221.0 640.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M812,227.0 812,202.0 944.0,202.0 944.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M812,229.0 L808,221.0 816,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M662,227.0 662,177.0 947.0,177.0 947.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M947.0,229.0 L951.0,221.0 943.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M662,227.0 662,152.0 1100.0,152.0 1100.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,229.0 L1104.0,221.0 1096.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M1262,227.0 1262,177.0 1547.0,177.0 1547.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1262,229.0 L1258,221.0 1266,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M1412,227.0 1412,202.0 1544.0,202.0 1544.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1412,229.0 L1408,221.0 1416,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M1112,227.0 1112,152.0 1550.0,152.0 1550.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1550.0,229.0 L1554.0,221.0 1546.0,221.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options = {'compact': True, 'bg': '#09a3d5',\n",
    "           'color': 'white', 'font': 'Source Sans Pro'}\n",
    "displacy.render(doc, style='dep', options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распознавание именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: linear-gradient(90deg, #aa9cfc, #fc9ce7); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying U.K. startup for $1 billion</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)'}\n",
    "options = {'ents': ['ORG'], 'colors': colors}\n",
    "displacy.render(doc, style='ent', options=options, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">But \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is starting from behind. The company made a late push\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "into hardware, and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s Siri, available on \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    iPhones\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "’s \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Alexa\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "software, which runs on its \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Echo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Dot\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " devices, have clear leads in\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    \n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "consumer adoption.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"But Google is starting from behind. The company made a late push\n",
    "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
    "software, which runs on its Echo and Dot devices, have clear leads in\n",
    "consumer adoption.\"\"\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Многовато ошибок*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка сходства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.0\n",
      "dog banana 0.0\n",
      "cat dog 0.0\n",
      "cat cat 1.0\n",
      "cat banana -0.044681177\n",
      "banana dog -7.828739e+17\n",
      "banana cat -8.242222e+17\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')  # make sure to use larger model!\n",
    "tokens = nlp(u'dog cat banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple 1.0\n",
      "Apple is -0.05732172\n",
      "Apple looking -2.8055505e-21\n",
      "Apple at 0.0\n",
      "Apple buying -8.315521e+17\n",
      "Apple U.K. 0.042299896\n",
      "Apple startup -2.2416173e-21\n",
      "Apple for -0.057875555\n",
      "Apple $ -0.036178503\n",
      "Apple 1 -9.8122005e+17\n",
      "Apple billion 1.8286364e-21\n",
      "is Apple 0.0\n",
      "is is 1.0\n",
      "is looking -1.3928241e+18\n",
      "is at 0.0\n",
      "is buying -3.56524e-21\n",
      "is U.K. 0.061713193\n",
      "is startup 0.060328145\n",
      "is for 0.0\n",
      "is $ 9.736638e+17\n",
      "is 1 -0.07760425\n",
      "is billion -9.078322e+17\n",
      "looking Apple -2.8055505e-21\n",
      "looking is -1.3928241e+18\n",
      "looking looking 1.0\n",
      "looking at -3.2815228e-21\n",
      "looking buying 0.0\n",
      "looking U.K. -1.0278183e+18\n",
      "looking startup 0.0\n",
      "looking for -0.076234676\n",
      "looking $ -2.5833796e-21\n",
      "looking 1 -0.070065476\n",
      "looking billion 0.044432882\n",
      "at Apple 0.0\n",
      "at is -1.2367909e+18\n",
      "at looking 0.0\n",
      "at at 1.0\n",
      "at buying 0.0\n",
      "at U.K. 0.0\n",
      "at startup 8.921919e+17\n",
      "at for -0.06769437\n",
      "at $ 0.0\n",
      "at 1 1.1476879e+18\n",
      "at billion 0.0\n",
      "buying Apple -8.315521e+17\n",
      "buying is -0.06576707\n",
      "buying looking 0.0\n",
      "buying at 0.0\n",
      "buying buying 1.0\n",
      "buying U.K. 0.0\n",
      "buying startup -0.04744282\n",
      "buying for 0.0\n",
      "buying $ -7.657017e+17\n",
      "buying 1 0.0\n",
      "buying billion 7.1393084e+17\n",
      "U.K. Apple 0.042299896\n",
      "U.K. is 0.0\n",
      "U.K. looking 0.0\n",
      "U.K. at 0.0\n",
      "U.K. buying 0.0\n",
      "U.K. U.K. 1.0\n",
      "U.K. startup -2.41335e-21\n",
      "U.K. for 1.1494068e+18\n",
      "U.K. $ -2.1114934e-21\n",
      "U.K. 1 0.0\n",
      "U.K. billion -6.699242e+17\n",
      "startup Apple -2.2416173e-21\n",
      "startup is 0.060328145\n",
      "startup looking 0.0\n",
      "startup at 8.921919e+17\n",
      "startup buying -2.5718804e-21\n",
      "startup U.K. -2.41335e-21\n",
      "startup startup 1.0\n",
      "startup for -3.3019933e-21\n",
      "startup $ 2.0641042e-21\n",
      "startup 1 0.0\n",
      "startup billion 0.035501596\n",
      "for Apple -0.057875555\n",
      "for is 0.0\n",
      "for looking -1.4062816e+18\n",
      "for at -0.06769437\n",
      "for buying 0.0\n",
      "for U.K. 1.1494068e+18\n",
      "for startup -3.3019933e-21\n",
      "for for 1.0\n",
      "for $ 2.888987e-21\n",
      "for 1 0.0\n",
      "for billion -9.166036e+17\n",
      "$ Apple -0.036178503\n",
      "$ is 9.736638e+17\n",
      "$ looking -2.5833796e-21\n",
      "$ at 2.2939726e-21\n",
      "$ buying 0.0\n",
      "$ U.K. 0.0\n",
      "$ startup 2.0641042e-21\n",
      "$ for 0.053292405\n",
      "$ $ 1.0\n",
      "$ 1 0.048979785\n",
      "$ billion 0.0\n",
      "1 Apple -9.8122005e+17\n",
      "1 is -1.4315457e+18\n",
      "1 looking -0.070065476\n",
      "1 at 1.1476879e+18\n",
      "1 buying 0.0\n",
      "1 U.K. 0.0\n",
      "1 startup 0.0\n",
      "1 for 0.0\n",
      "1 $ 2.6551995e-21\n",
      "1 1 1.0\n",
      "1 billion -2.4756752e-21\n",
      "billion Apple 0.03373239\n",
      "billion is -9.078322e+17\n",
      "billion looking 0.044432882\n",
      "billion at 0.0\n",
      "billion buying 7.1393084e+17\n",
      "billion U.K. 1.9687302e-21\n",
      "billion startup 0.035501596\n",
      "billion for -9.166036e+17\n",
      "billion $ 0.0\n",
      "billion 1 -2.4756752e-21\n",
      "billion billion 1.0\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> banana -7.7179016e+17\n",
      "pasta <-> hippo -0.04038117\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(u\"Apple and banana are similar. Pasta and hippo aren't.\")\n",
    "\n",
    "apple = doc[0]\n",
    "banana = doc[2]\n",
    "pasta = doc[6]\n",
    "hippo = doc[8]\n",
    "\n",
    "print('apple <-> banana', apple.similarity(banana))\n",
    "print('pasta <-> hippo', pasta.similarity(hippo))\n",
    "print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Еще несколько примеров задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление имен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In 1950, [REDACTED] published his famous article \"Computing Machinery and Intelligence\". In 1957, [REDACTED] \n",
      "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка английской NLP-модели\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    " \n",
    "# Если токен является именем, заменяем его словом \"REDACTED\" \n",
    "def replace_name_with_placeholder(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED] \"\n",
    "    else:\n",
    "        return token.string\n",
    " \n",
    " # Проверка всех сущностей\n",
    "def scrub(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        ent.merge()\n",
    "    tokens = map(replace_name_with_placeholder, doc)\n",
    "    return \"\".join(tokens)\n",
    " \n",
    "s = \"\"\"\n",
    "In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\". In 1957, Noam Chomsky’s \n",
    "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
    "\"\"\"\n",
    " \n",
    "print(scrub(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извлечение фактов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the things I know about London:\n",
      " - the capital and most populous city of England and  the United Kingdom.  \n",
      "\n",
      " - a major settlement  for two millennia.  \n"
     ]
    }
   ],
   "source": [
    "import textacy.extract\n",
    " \n",
    "# Загрузка английской NLP-модели\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    " \n",
    "# Текст для анализа\n",
    "text = \"\"\"London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\n",
    "\"\"\"\n",
    " \n",
    "# Анализ\n",
    "doc = nlp(text)\n",
    " \n",
    "# Извлечение полуструктурированных выражений со словом London\n",
    "statements = textacy.extract.semistructured_statements(doc, \"London\")\n",
    " \n",
    "# Вывод результатов\n",
    "print(\"Here are the things I know about London:\")\n",
    " \n",
    "for statement in statements:\n",
    "    subject, verb, fact = statement\n",
    "    print(f\" - {fact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автодополнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "river thames\n",
      "major settlement\n",
      "south east\n",
      "united kingdom\n",
      "great britain\n",
      "most populous city\n",
      "two millennia\n"
     ]
    }
   ],
   "source": [
    "import textacy.extract\n",
    " \n",
    "# Загрузка английской NLP-модели\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    " \n",
    "# Текст для анализа\n",
    "text = \"\"\"London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\n",
    "London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\n",
    "London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\"\"\"\n",
    " \n",
    "# Анализ\n",
    "doc = nlp(text)\n",
    " \n",
    "# Извлечение фрагментов\n",
    "noun_chunks = textacy.extract.noun_chunks(doc, min_freq=3)\n",
    " \n",
    "# Перевод в нижний регистр\n",
    "noun_chunks = map(str, noun_chunks)\n",
    "noun_chunks = map(str.lower, noun_chunks)\n",
    " \n",
    "# вывод всех фрагментов, состоящих из 2 слов и более\n",
    "for noun_chunk in set(noun_chunks):\n",
    "    if len(noun_chunk.split(\" \")) > 1:\n",
    "        print(noun_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 6.0)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Билиотека sklearn позволяет легко загрузить коллекцию документов 20-newsgroups, часто используемую для сравнения методов классификации и кластеризации текстов. Загрузим документы из двух категорий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset...\n",
      "1403 documents\n",
      "2 categories: ['talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading 20 newsgroups dataset...\")\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=['talk.religion.misc', 'talk.politics.misc'],\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories:\" % len(dataset.target_names), dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый документ представлен сырым текстом (plain text). Например, так выглядит первый документ, и он про политику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: bskendig@netcom.com (Brian Kendig)\n",
      "Subject: Re: Is it good that Jesus died?\n",
      "Organization: Starfleet Headquarters: San Francisco\n",
      "Lines: 15\n",
      "\n",
      "sandvik@newton.apple.com (Kent Sandvik) writes:\n",
      ">\n",
      ">I've done all those things, and I've regretted it, and I learned \n",
      ">a lesson or two. So far an aspirin, a good talk with your wife,\n",
      ">or a one week vacation has cured me -- no need for group therapy\n",
      ">or strange religions!\n",
      "\n",
      "Um, Kent... just what *have* you been doing with his wife?!?  ;-D\n",
      "\n",
      "-- \n",
      "_/_/_/  Brian Kendig                             Je ne suis fait comme aucun\n",
      "/_/_/  bskendig@netcom.com                de ceux que j'ai vus; j'ose croire\n",
      "_/_/                            n'etre fait comme aucun de ceux qui existent.\n",
      "  /  The meaning of life     Si je ne vaux pas mieux, au moins je suis autre.\n",
      " /    is that it ends.                                           -- Rousseau\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset.data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk.politics.misc\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names[dataset.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы сбалансированы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[775 628]\n"
     ]
    }
   ],
   "source": [
    "Y = np.array(dataset.target)\n",
    "print(np.bincount(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы обучать модели классификации, нам нужно сначала описать каждый документ каким-то набором признаков. Будем считать, что каждый документ - это мешок слов (порядок слов не важен), подсчитаем частоту каждого слова в документе (Term frequency) и оштрафуем общеупотребительные слова с помощью обратной документной частоты (Inverted document frequency). Такой подход называется TfIdf и часто используется на практике для работы с текстами.\n",
    "\n",
    "Реализация в sklearn позволяет также указать параметры фильтрации словаря. В данном случае мы выбрасываем слова, которые встречаются более чем в 70% документов, менее чем в 10 документах, а также английские стоп-слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.7, min_df=10, stop_words='english')\n",
    "X = vectorizer.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем матрицу объект-признак. Каждый документ описывается важностью слов (tf-idf скором), ставших признаками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1403, 3854)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно учить модели! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели и оценка качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Важно: чтобы иметь возможность честно измерить качество модели, разобьем выборку на обучающую и контрольную. На обучении натренируем SVM, а не контроле измерим точность классификации (accuracy). Для этого есть удобный метод:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1262, 3854) (1262,)\n",
      "(141, 3854) (141,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anastasiazuhba/anaconda3/envs/Py3ML/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9716312056737588"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SGDClassifier(loss=\"hinge\", n_iter=200).fit(X_train, Y_train)\n",
    "svm.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет accuracy другим способом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9716312056737588"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = svm.predict(X_test)\n",
    "metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичным образом в sklearn.metrics реализовано множество других полезных показателей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.984375\n",
      "Precision: 0.9545454545454546\n",
      "F1-score: 0.9692307692307692\n",
      "ROC_AUC: 0.9727069805194805\n"
     ]
    }
   ],
   "source": [
    "print('Recall:', metrics.recall_score(Y_test, Y_pred))\n",
    "print('Precision:', metrics.precision_score(Y_test, Y_pred))\n",
    "print('F1-score:', metrics.f1_score(Y_test, Y_pred))\n",
    "print('ROC_AUC:', metrics.roc_auc_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось очень неплохое качество классификации! TfIdf + SVM является классическим бейзлайном для классификации текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можно сделать дальше:\n",
    "* Сделать кроссвалидацию и подобрать гиперпараметры.\n",
    "* Посчитать confusion-matrix http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html \n",
    "* Нарисовать ROC-кривые, например, так: http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "* Обучить многоклассовый SVM (в оригинальном датасете 20 категорий, а не 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия для той же задачи\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l1 .............................................\n",
      "[CV] .................... C=0.001, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.993939 -   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.997450 -   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.978075 -   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.986845 -   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.976562 -   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.993878 -   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.990385 -   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.994898 -   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.992411 -   0.0s\n",
      "[CV] C=0.001, penalty=l2 .............................................\n",
      "[CV] .................... C=0.001, penalty=l2, score=0.969075 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l1 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l1, score=0.500000 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.996970 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.997960 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.980343 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.987351 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.976310 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.994643 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.991720 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.993878 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.993929 -   0.0s\n",
      "[CV] C=0.01, penalty=l2 ..............................................\n",
      "[CV] ..................... C=0.01, penalty=l2, score=0.969854 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.664646 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.606068 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.698337 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.756135 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.702873 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.793367 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.666132 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.606633 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.724766 -   0.0s\n",
      "[CV] C=0.1, penalty=l1 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l1, score=0.654496 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.998485 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.998470 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.982107 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.988363 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.977571 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.996429 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.993056 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.996429 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.995446 -   0.0s\n",
      "[CV] C=0.1, penalty=l2 ...............................................\n",
      "[CV] ...................... C=0.1, penalty=l2, score=0.974012 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.972222 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.985977 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.962198 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.924361 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.942288 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.973980 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.971421 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.981760 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.976600 -   0.0s\n",
      "[CV] C=1, penalty=l1 .................................................\n",
      "[CV] ........................ C=1, penalty=l1, score=0.955042 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........................ C=1, penalty=l2, score=0.998485 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........................ C=1, penalty=l2, score=0.999745 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........................ C=1, penalty=l2, score=0.984375 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........................ C=1, penalty=l2, score=0.990640 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........................ C=1, penalty=l2, score=0.985131 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........................ C=1, penalty=l2, score=0.999490 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........................ C=1, penalty=l2, score=0.994658 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........................ C=1, penalty=l2, score=0.999235 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ C=1, penalty=l2, score=0.997217 -   0.0s\n",
      "[CV] C=1, penalty=l2 .................................................\n",
      "[CV] ........................ C=1, penalty=l2, score=0.984407 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.995455 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.998470 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.982359 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.977991 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.968750 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.998724 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.982105 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.989031 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.992664 -   0.0s\n",
      "[CV] C=10, penalty=l1 ................................................\n",
      "[CV] ....................... C=10, penalty=l1, score=0.972713 -   0.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=0.997727 -   0.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=1.000000 -   0.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=0.989163 -   0.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=0.991652 -   0.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=0.991179 -   0.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=0.999235 -   0.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=0.997329 -   0.1s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=0.999745 -   0.1s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=0.997470 -   0.0s\n",
      "[CV] C=10, penalty=l2 ................................................\n",
      "[CV] ....................... C=10, penalty=l2, score=0.990904 -   0.0s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.992929 -   0.0s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.997705 -   0.1s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.984627 -   0.1s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.982039 -   0.1s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.971774 -   0.1s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.998980 -   0.0s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.985310 -   0.0s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.992857 -   0.0s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.992664 -   0.0s\n",
      "[CV] C=100, penalty=l1 ...............................................\n",
      "[CV] ...................... C=100, penalty=l1, score=0.976351 -   0.2s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=0.996717 -   0.1s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=1.000000 -   0.1s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=0.989667 -   0.1s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=0.992411 -   0.1s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=0.992692 -   0.1s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=0.999490 -   0.1s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=0.997596 -   0.1s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=0.999745 -   0.1s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=0.997470 -   0.1s\n",
      "[CV] C=100, penalty=l2 ...............................................\n",
      "[CV] ...................... C=100, penalty=l2, score=0.993503 -   0.1s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.994949 -   0.1s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.999235 -   0.1s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.979839 -   0.1s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.983304 -   0.1s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.977571 -   0.0s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.999745 -   0.0s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.991720 -   0.1s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.991071 -   0.1s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.993676 -   0.1s\n",
      "[CV] C=1000, penalty=l1 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l1, score=0.988046 -   0.0s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l2, score=0.996970 -   0.1s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l2, score=1.000000 -   0.1s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l2, score=0.990423 -   0.1s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l2, score=0.992917 -   0.1s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l2, score=0.992440 -   0.1s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l2, score=0.999235 -   0.1s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l2, score=0.998130 -   0.1s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1000, penalty=l2, score=0.999745 -   0.1s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l2, score=0.996964 -   0.1s\n",
      "[CV] C=1000, penalty=l2 ..............................................\n",
      "[CV] ..................... C=1000, penalty=l2, score=0.993503 -   0.2s\n",
      "Best score is:  0.9960365617070364\n",
      "Best parametrs:\n",
      "C : 1000\n",
      "class_weight : None\n",
      "dual : False\n",
      "fit_intercept : True\n",
      "intercept_scaling : 1\n",
      "max_iter : 100\n",
      "multi_class : ovr\n",
      "n_jobs : 1\n",
      "penalty : l2\n",
      "random_state : None\n",
      "solver : liblinear\n",
      "tol : 0.0001\n",
      "verbose : 0\n",
      "warm_start : False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 140 out of 140 | elapsed:    4.9s finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000], \"penalty\": [\"l1\", \"l2\"]}\n",
    "model = LogisticRegression()\n",
    "cv = cross_validation.KFold(len(Y_train), n_folds=10, shuffle=True, random_state=1234)\n",
    "gs = grid_search.GridSearchCV(model, param_grid, scoring=\"roc_auc\", cv=cv, verbose=10)\n",
    "gs.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best score is: \", gs.best_score_)\n",
    "print(\"Best parametrs:\")\n",
    "\n",
    "best_params = gs.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(best_params.keys()):\n",
    "    print(param_name, \":\", best_params[param_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9645390070921985"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=1000, penalty='l2').fit(X_train, Y_train)\n",
    "Y_lr = lr.predict(X_test)\n",
    "metrics.accuracy_score(Y_test, Y_lr)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
